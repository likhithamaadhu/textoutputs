Welcome to jacks and shells where we explore basics and useful tips for getting up and running on GK tensor processing units or tpus are chips that Google created to speed up machine learning tasks. Tpus make machine learning model work more efficiently by using hardware that is purpose. Build to accelerate ML computation pattern. For example, matrix multiplication or communication amongst workers in the cluster tpus also have built in high bandwidth memory. This allows it to run even the biggest and most demanding models. Keep your training supercomputer features hundreds of TPU chips connected together with their high speed. Chip to chip interconnect called ICI ICI links, coupled with TPU software stack allows you to scale your workload from one chip to much larger. Scale with ease. Based on your workload, you can choose. To create a. Specified subset of this supercomputer as instances referred to as TPU Slice. You can use. A single GPU slice or even multiple TPU slices to accelerate your large scale training jobs, or use smaller slice shapes down to a single VM with a single chip for intrax. Google Cloud Tpus are highly cost efficient for training of large language models. Visual transformer based architecture. Diffusion based generative models and recommendation system. However, until now, orchestrating large scale AI workloads with cloud tibus have been a cumbersome process. Users have to perform a significant amount of manual work to handle failure, logging, monitoring, and other basic operations. We have heard your concerns and we are. Here to help. With the announcement of cloud Tpus in Google Kubernetes engine, we are providing the much anticipated support for GKE for cloud TPU. V4 and the latest field generation, cloud TPU V5E. With this it will be easier than ever to develop and manage large scale AI workloads on cloud tpus, cloud tpus in GKE combine the price to performance of Google's custom designed GPUs with the power of GTK. This means you can use the simplicity of GKE. You run your AI workloads and take advantage of GTK features like auto scaling, auto provisioning, auto repair and auto upgrade to ease the burden of day two operations and manage your resources more efficiently. You can also combine the power of Tpus and GK ecosystems using tools like weights and bias is launched and managed services for Prometheus with tpus in GTK. If you're already. Using Kubernetes to run your web stateless stateful batch. Or other workloads. You already know how great it is to have. A single platform. To manage everything now with the general availability of cloud tpus on jke, you can get all the same benefits for your AI workloads, cloud TPU node pools can be auto provisioned and. To scale so you only use the resources you need and never have to worry about over or under provisioning and you can choose between on demand spot and reserve cloud TP instances to fit your budget and workload. Needs GTK node. Pool groups compute resources that are managed by Kubernetes to schedule workloads on single. Most TPU slice load pools can scale like normal load pools. A TPU slice is an interconnected group of 1 or more TPU. Places a TPU device is another name for a single TPU VM or host. For TPU V4 this is. A V4 by 8. Which has 8 Tensor Core, 4 chips and the topology of two by two by one TPU topology refers to how the tpus are connected together. Tpu's can be connected in different ways. Based on application needs a multi host TPU slices single atomic unit. That is if any of the TPU devices within the node group fails, the entire slice cannot be used. A multi hosted view slice also can be auto scaled after provisioning the TPU chips in the slice are interconnected in a certain physical mesh defined by the topology. Therefore a multi host GPU slice in GKE is represented with a single atomic Kubernetes notable. For example, a TPU V432 has a topology of 2 by 2 by 4. Meaning that there are 4 TPU devices connected with ispin interconnect. Each device has four TPU chips. Each chip has two tensor cores. Queue is an open source project started by the Kubernetes Batch working group for fair sharing and quota management for batch and ML workloads and coordinates. You can use queue for scheduling jobs on TPU notes TPU logs in JKA are automatically collected by cloud logging and available in Log Explorer. You can write your log to standard error and standard out. You can also forward these logs to pubsub, cloud storage, big query or any other third party platform. This has generally been a challenging task in TPU workloads. A workload in TPU is spread across many VMS. A centralized logging system will help speed up the finding and debugging of potential issues. Errors are automatically detected in logs and visualized as histograms. To give a view of historical frequency. This lets you quickly identify common problems and the history of the error. Many metrics are exported to cloud monitor like duty cycle, which indicates percent of time that TPU is being actively utilized. This helps determine the efficiency of our TPE usage. These metrics can also be used to configure when it is horizontal, autoscaling. Tpu V5 E is our latest. Offering in our TPU AI supercomputer family, Cloud TPU V5 E is designed to enable the next generation of AI. It's our most cost efficient, scalable and versatile cloud TPU acceleration platform. Today, TPV 5 offers up to two times higher training performance and up to 2.5 times inference performance per dollar compared to TPU V4 for alms and generative AI model. In this video, we talked about how TPU and GK work together to give you the power to build the AML platform of the future. To learn more about Tpus in GKE, click the links in the description below. 

 