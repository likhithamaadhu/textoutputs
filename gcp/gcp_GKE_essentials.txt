Welcome to gksonshells where we explore Basics and useful tips for getting up and running on gke. Processing units or tpus are chips that Google created to speed up machine learning tasks tpus make machine learning model work more efficiently by using Hardware that is purpose built to accelerate. Ml computation patterns. For example matrix multiplication or communication amongst workers in a cluster tpus also have built-in high bandwidth memory. This allows it to run even the biggest and most demanding models. I keep you training supercomputer features hundreds of Debut chips connected together with a high speed cheap to chip interconnect called ICI. Lynx coupled with TPU software stack allows you to scale your workload from one chip to much larger scale with ease based on your workload. You can choose to create a specified subset of this super computer as instances refer to as TPU slice.You can use a single TPU slice or even multiple TPU slices to accelerate your large-scale training jobs or use smaller slice shapes down to a single VM with a single chip for inference Google Cloud tpus are highly cost efficient for training of large language models Vision Transformer based architecture diffusion-based generative models and recommendation systems. However until now orchestrating large-scale ai workload with Cloud tpus have been a cumbersome process.Users have to perform a significant amount of manual work to handle failure logging monitoring and other basic operations. We have your concerns and we're here to help. With the announcement of cloud tpus in Google kubernetes engine. We are providing the much anticipated support for gke for cloud TPU V4 and the latest field generation Cloud TPU v5e with this it will be easier than ever to develop and manage large-scale AI workloads on cloud tpus cloud tpus in gke combine the price to Performance of Google's custom design tpus with the power of gke this means you can use the Simplicity of gke to run your AI workloads and take advantage of GK features like Auto scaling Auto provisioning auto repair and auto upgrade to ease the burden of day to operations and manage your resources more efficiently.You can also combine the power of tpus and gke ecosystems using tools like weights and bass is launched and managed services for Prometheus with tpus in GK. If you're already using kubernetes to run your web stateless stateful batch or other workloads. You already know how great it is to have a single platform to manage everything now with the general availability of cloud tpus on gke you can get all the same benefits for your AI workloads Cloud TPU note pools can be Auto provisioned and autoscale. So you only use the resources you need and never have to worry about over or under provisioning and you can choose between on-demands spot and Reserve Cloud TV instances to feed your budget and workload needs GK note pool groups compute resources that are managed by kubernetes to schedule workloads on single host TPU slice those pools can scale like normal. I think you slice it's intermitted group.Of one or more TPU devices a TPU device is another name for a single TPU VM or hose for teamview V4. This is the V4 by 8, which has 8 tensor core 4 chips and the topology of 2 by 2 by 1. TPU topology refers to how the tpus are connected together. Tpus can be connected in different ways based on application needs a multi-host tip use slice a single Atomic unit. That is if any of the TPU devices within the node group fails the entire slice cannot beers.A multi-hosted view slice also can be autoscaled after provisioning. The TPU chips in the slice are interconnected in a certain physical mesh defined by the topology therefore a multi-hosted view slice in gke is represented with a single Atomic kubernetes Note 4. For example a TPU V4 32 has a topology of 2 by 2 by 4. Meaning that there are four TPU devices connected with ispit interconnect.Each device has four TPU chips. Each chip has two tensor cores Q is the open source project started by the kubernetes batch working group for fair sharing and quota management for batch and ML workloads and kubernetes. You can use Q for scheduling jobs on TPU notes TPU logs in gke automatically collected by Cloud logging and available

in log Explorer. You can write your log to standard error and standard out. You can also forward this logs to pub/sub cloud storage bigquery or any other third party platform. This has generally been a challenging task in TPU workloads. A workloading TPU is spread across many VMS a centralized logging system will help speed up the finding and debugging of potential issues errors are automatically detected in logs and visualized as histograms to give a view of historical frequency.This lets you quickly identify common problems and the history of the error. Many metrics are exported to Cloud monitor like duty cycle which indicates percent of time. The TPU is being actively utilized this helps determine the efficiency of our tpe usage. This metrics can also be used to configure kubernetes horizontal Auto scaling TPU v5e is our latest offering in our TPU AI supercomputer family Cloud TPU v5e is designed to enable the next generation of AI applications. It's our most cost efficient scalable and versatile Cloud tpu-a acceleration platform today TPU v5e offers up to two times higher training performance and up to 2.5 times inference performance per dollar compared to tpuv4 for lengths and generative AI model.In this video we talked about how TPU and GK worked together to give you the power to build the AML plug from of the future. Find more about tpus and dke click the links in the description below.