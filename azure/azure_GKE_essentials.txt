Welcome to GKSN shows where we explore basics and useful tips for getting up and running on GKE. Tensor Processing Units or Tpus are chips that Google created to speed up machine learning tasks. Tpus make machine learning model work more efficiently by using hardware that is purpose built to accelerate ML computation patterns, for example matrix multiplication or communication amongst workers in a cluster. CPUs also have built in high bandwidth memory. This allows it to run even the biggest and most demanding models. A TPU training supercomputer features hundreds of CPU chips connected together with a high speed chip to chip interconnect called ICI. ICI links coupled with TPU software stack allows you to scale your workload from one chip to much larger scale with ease. Based on your workload, you can choose to create a specified subset of this supercomputer as instances referred to as TPU slice. You can use a single TPU slice or even multiple TPU slices to accelerate your large scale training jobs or use smaller slice shapes down to a single VM with a single chip for infrax. Google Cloud Tpus are highly cost efficient for training of large language models, vision transformer based architecture, diffusion based generative models and recommendation systems. However, until now orchestrating large scale AI work node with Cloud Tpus have been a cumbersome process. Users have to perform a significant amount of manual work to handle failure logging, monitoring and other basic operations. We have hardier concerns and we're here to help. With the announcement of Cloud CPUs in Google Kubernetes Engine, we're providing the much anticipated support for GKE for cloudtpu V4 and the latest field generation cloudtpu V5E. With this it will be easier than ever to develop and manage large scale AI workloads on Cloudtps. Cloudtps in GKE combine the price to performance of Google's custom designed CPUs with the power of GKE. This means you can use the simplicity of GKE to run your AI workloads and take advantage of GKE features like Auto Scaling, Auto Provisioning, Auto Repair, and Auto Upgrade to ease the burden of data operations and manage your resources more efficiently. You can also combine the power of Tpus and GK ecosystem using tools like Wits and BASS is launched and manage services for Prometheus with Tpus in GK. If you're already using Kubernetes to run your web stateless, stateful, batch or other work nodes, you already know how great it is to have a single platform to manage everything. Now with the general availability of cloud CPUs on JKE, you can get all the same benefits for your AI workloads. Cloud CPU node pools can be auto provisioned and auto scale so you only use the resources you need and never have to worry about over or under provisioning. And you can choose between on demand spot and reserve Cloud TP instances to fit your budget and workload needs. GK node pool groups compute resources that are managed by Kubernetes to schedule workloads on single host. TP slice node pools can scale like normal node pools. A TPU slice is an interconnected group of 1 or more TPU devices. A TPU device is another name for a single TPU VM or host. For TPU V4, this is a V4 by 8 which has 8 tensor core, 4 chips and the topology of two by two by one. TPU topology refers to how the Tpu's are connected together. Tpus can be connected in different ways based on application needs. A multi host TPU slice is a single atomic unit. That is, if any of the TPU devices within the node group fails, the entire slice cannot be used. A multi host TPU slice also can be autoscaled after provisioning. The TPU chips in the slice are interconnected in a certain physical mesh defined by the topology. Therefore, a multi host TPU slice in GKE is represented with a single atomic Kubernetes notebook. For example, a TPUV 432 has a topology of 2 by 2 by 4, meaning that there are four CPU devices connected with I speed interconnect. Each device has four CPU chips. Each chip has two tensor cores. Q is an open source project started by the Kubernetes Batch Working Group for fair sharing and quota management for batch and annual workloads and Kubernetes. You can use Q for scheduling jobs on TPU. Notes TPU logs in JK are automatically collected by Cloud Logging and available in Log Explorer. You can write your log to standard Error and standard out. You can also forward these logs to pops up Cloud Storage, Big Query, or any other third party platform. This has generally been a challenging task in TP workloads. A workload in TPU is spread across many Vms. A centralized logging system will help speed up the finding and debugging of potential issues. Errors are automatically detected in logs and visualized as histograms to give a view of historical frequency. This lets you quickly identify common problems and the history of the error. Many metrics are exported to cloud monitoring like duty cycle which indicates percent of time that TPU is being actively utilized. This helps determine the efficiency of our TP usage. This metrics can also be used to configure Kubernetes horizontal photo scaling. TPU V5 E is our latest offering in our TPU AI supercomputer family. Cloud TPU V5 E is designed to enable the next generation of AI applications. It's our most cost efficient, scalable and versatile Cloud TPUA acceleration platform. Today, TPU V5 E offers up to two times higher training performance and up to 2.5 times inference performance per dollar compared to TPU V4 for Llms and generative AI model. In this video we talked about how TPU and GK work together to give you the power to build the AML platform of the future. To learn more about Tpu's in DKE, click the links in the description below.