Welcome to GKS and shows where we explore basics and useful tips for getting up and running on GK tensor processing units or TP US are chips that Google created to speed up machine learning tasks. TP US make machine learning model work more efficiently by using hardware that is purpose built to accelerate ML computation powers. For example, matrix multiplication or communication amongst workers in a cluster TP US also have built in high bandwidth memory. This allows it to run even the biggest and most demanding models. A TPU training supercomputer features hundreds of TP chips connected together with a high speed chip to chip interconnect called IC IC I links coupled with TP software stack allows you to scale your workload from one chip to a much larger scale with ease based on your workload. You can choose to create a specified subset of this supercomputer as instances referred to as TU slice. You can use a single TPU slice or even multiple TPU slices to accelerate your large scale training jobs or use smaller slice shapes down to a single VM with a single chip for inverts Google cloud TP US are highly cost efficient for training of large language models, vision transformer based architecture, diffusion based generative models and recommendation systems. However, until now orchestrating large scale A I work mode with cloud TV. US have been a cumbersome process. Users had to perform a significant amount of manual work to handle failure, logging, monitoring and other basic operations. We have heard your concerns and we are here to help with the announcement of cloud TP US in Google engine. We are providing the much anticipated support for GKE for cloud T PV four and the latest fifth generation cloud T PV five E. With this, it will be easier than ever to develop and manage large scale a workloads on cloud TP US, cloud T US and GKE combine the price to performance of Google's custom design tps with the power of GK. This means you can use the simplicity of GKE to run your air workloads and take advantage of GK features like auto scaling, auto provisioning, auto repair and auto upgrade to ease the burden of data operations and manage your resources more efficiently. You can also combine the power of TP US and GK ecosystem using tools like weights and basses launch and manage services for with TP US NGK. If you're already using COTI to run your web stateless state, full batch or other word notes, you already know how great it is to have a single platform to manage everything. Now with the general availability of cloud tps on GKE, you can get all the same benefits for your A I workloads. Cloud TP. Note pools can be auto provisioned and auto scale. So you only use the resources you need and never have to worry about over or under provisioning. And you can choose between on demand spot and reserve cloud TP instances to fit your budget and workload needs. GK Note pool groups compute

sources that are managed by kubernetes to schedule workloads on single host TPU slice load poles can scale like normal load poles. A TPU slice is an interconnected group of one or more TU devices. A TU device is another name for a single TUVM or host for PUV four. This is a V four by eight which has 8 10 core four chips and the topology of two by two by one TPU topology refers to how the T US are connected together TP US can be connected in different ways. Based on application needs a multi host Tu slice is a single atomic unit that is if any of the TP devices within the node group fails, the entire slice cannot be used. A multi hosted view slice also can be auto scaled after provisioning the TU chips in the slice are interconnected in a certain physical mesh defined by the topology. Therefore, a multi host Ti U slice in GKE is represented with a single atomic coin. It is notable for example, a TUV 4 32 has a topology of two by two by four. Meaning that there are four TP devices connected with I speeded interconnect. Each device has four TU chips. Each chip has two tensor cores. Q is an open source project started by the Cober batch working group for fair sharing and quota management for batch and work loads on COTIS. You can use Q for scheduling jobs on TPU notes. TU logs in GK are automatically collected by cloud logging and available in log explorer. You can write your log to standard error and standard out. You can also forward these logs to pops up cloud storage, Bigquery or any other third party platform. This has generally been a challenging task in TP workloads. A workload in TPU is spread across many V MS. A centralized logging system will help speed up the finding and debugging of potential issues. Errors are automatically detected in logs and visualized as histograms to give a view of historical frequency. This lets you quickly identify common problems and the history of the error. Many metrics are exported to cloud monitor like beauty cycle which indicates a percent of time that TPU is being actively utilized. This helps determine the efficiency of our TPU usage. These metrics can also be used to configure Ken's horizontal photo scaling TPUV five E is our latest offering in our TPUA I supercomputer family cloud T PV IB is designed to enable the next generation of A I applications. It's our most cost efficient scalable and versatile cloud TPU A acceleration platform. Today TPUV five E offers up to two times higher training performance and up to 2.5 times inverness performance per dollar compared to TPUV four for LLS and generative A I model. In this video, we talked about how TPU and GK work together to give you the power to build the AM L platform of the future to learn more about TP US and GKE, click the links in the description below.